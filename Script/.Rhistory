knitr::opts_chunk$set(echo = TRUE)
# Load libraries
install.packages("pacman")
library(pacman)
p_load(caret, lattice, readr, Metrics, corrplot, e1071, mlr, recipes, ggplot2, C50, party, reshape, dplyr,
markdown, ggpubr, tidyr, hydroGOF, BBmisc, tidyverse, textclean,
inum, doParallel, Hmisc, caretEnsemble, mboost, cluster, ade4, factoextra, asbio, SatMatch, FactoMineR, fpc,
e1071, randomForest, rstudioapi, MASS, ParamHelpers, mlr
)
# Enable parallel computing
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
# Loading dataset(s)
current_path = getActiveDocumentContext()$path
setwd(dirname(current_path))
# Loading dataset(s)
current_path = getActiveDocumentContext()$path
setwd(dirname(current_path))
ExistProd <- read.csv("existingproductattributes2017.csv")
NewProd   <- read.csv("newproductattributes2017.csv")
#Removing them:
Duplicates <- ExistProd[c(32:41),]
Duplicates[3,3] <- sum(Duplicates[3:10,3])/8
Duplicates <- Duplicates[1:3,]
ExistProd <- ExistProd[-c(32:41),]
ExistProd <- rbind(ExistProd, Duplicates)
#We proceed to get rid of the aforementioned columns in both df (they don't need to be the same for prediction, but it's good practice)
ExistProd <- ExistProd[,-which(names(ExistProd) %in% c("Price","BestSellersRank","ShippingWeight","ProductDepth","ProductWidth","ProductHeight"))]
which(ExistProd$ProductType == PC)
which(ExistProd$ProductType == "PC")
View(ExistProd)
View(ExistProd)
p
# Load libraries
install.packages("pacman")
# Load libraries
#install.packages("pacman")
library(pacman)
p_load(caret, lattice, readr, Metrics, corrplot, e1071, mlr, recipes, ggplot2, C50, party, reshape, dplyr,markdown, ggpubr, tidyr, hydroGOF, BBmisc, tidyverse, textclean, inum, doParallel, Hmisc, caretEnsemble, mboost, cluster, ade4, factoextra, asbio, SatMatch, FactoMineR, fpc,e1071, randomForest, rstudioapi, MASS, ParamHelpers, mlr)
# Enable parallel computing
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
# Loading dataset(s)
current_path = getActiveDocumentContext()$path
setwd(dirname(current_path))
ExistProd   <- read.csv("existingproductattributes2017.csv")
NewProd     <- read.csv("newproductattributes2017.csv")
NewProd_Raw <- read.csv("newproductattributes2017.csv")
install.packages("SaMatch")
# Load libraries
#install.packages("pacman")
library(pacman)
library(caret)
library(lattice)
library(readr)
library(Metrics)
library(corrplot)
library(e1071)
library(mlr)
library(recipes)
library(ggplot2)
library(C50)
library(party)
library(reshape)
library(dplyr)
library(markdown)
library(ggpubr)
library(tidyr)
library(hydroGOF)
library(BBmisc)
library(tidyverse)
library(textclean)
library(inum)
library(doParallel)
library(Hmisc)
library(caretEnsemble)
library(mboost)
library(cluster)
library(ade4)
library(factoextra)
library(asbio)
#library(SatMatch)
library(FactoMineR)
library(fpc)
library(e1071)
library(randomForest)
library(rstudioapi)
library(MASS)
library(ParamHelpers)
library(mlr)
# Enable parallel computing
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
# Loading dataset(s)
current_path = getActiveDocumentContext()$path
setwd(dirname(current_path))
ExistProd   <- read.csv("existingproductattributes2017.csv")
NewProd     <- read.csv("newproductattributes2017.csv")
NewProd_Raw <- read.csv("newproductattributes2017.csv")
setwd(C:/Users/Paul/Desktop/Ubiqum/Module 2/2.3 Multiple Regression/Git/Profitability)
# Loading dataset(s)
current_path = getActiveDocumentContext()$path
setwd("C:/Users/Paul/Desktop/Ubiqum/Module 2/2.3 Multiple Regression/Git/Profitability")
ExistProd   <- read.csv("existingproductattributes2017.csv")
NewProd     <- read.csv("newproductattributes2017.csv")
NewProd_Raw <- read.csv("newproductattributes2017.csv")
#We find lots of outliers in almost all columns. If we remove them, it will drastically change our dataset (already very small), and so we don't remove all of them.
ggplot(stack(ExistProd), aes(x=ind, y=values)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45)).
knitr::opts_chunk$set(echo = TRUE)
# Load libraries
#install.packages("pacman")
library(pacman)
library(caret)
library(lattice)
library(readr)
library(Metrics)
library(corrplot)
library(e1071)
library(mlr)
library(recipes)
library(ggplot2)
library(C50)
library(party)
library(reshape)
library(dplyr)
library(markdown)
library(ggpubr)
library(tidyr)
library(hydroGOF)
library(BBmisc)
library(tidyverse)
library(textclean)
library(inum)
library(doParallel)
library(Hmisc)
library(caretEnsemble)
library(mboost)
library(rstudioapi)
library(cluster)
library(ade4)
library(factoextra)
library(asbio)
#library(SatMatch)
library(FactoMineR)
library(fpc)
library(e1071)
library(randomForest)
library(MASS)
library(ParamHelpers)
library(mlr)
# Enable parallel computing
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
# Loading dataset(s)
#current_path = getActiveDocumentContext()$path
setwd("C:/Users/Paul/Desktop/Ubiqum/Module 2/2.3 Multiple Regression/Git/Profitability")
ExistProd   <- read.csv("existingproductattributes2017.csv")
NewProd     <- read.csv("newproductattributes2017.csv")
NewProd_Raw <- read.csv("newproductattributes2017.csv")
#Because we have worked with these products before, and because the dataset is small, we can observe duplicates just by looking at the df itself. We find out all 'ExtendedWarranty' products share the same values in everything (including exact number of reviews of all types) except for a small different in price. In a business sense, We conclude this is because warranties are sold alongside different products, only the same type of warranty is included, but will vary in price proportionally to the price itself. We should treat them as the same products and remove all but one.
#Showing the 6 duplicates:
#IMPORTANT NOTE: It's good practice to remove the columns by name instead of by column number, so that if we do changes in dataset later, and we reload these commands, we won't mess up unwanted columns.
sum(duplicated(ExistProd[,c(1,4:18)]))
#Removing them, but setting an average price for the remaining product of Warranty:
#ExistProd <- ExistProd[-c(35:41),]
Duplicates <- ExistProd[c(32:41),]
Duplicates[3,3] <- sum(Duplicates[3:10,3])/8
Duplicates <- Duplicates[1:3,]
ExistProd  <- ExistProd[-c(32:41),]
ExistProd  <- rbind(ExistProd, Duplicates)
#We checked in the previous project that some columns were not useful and just added noise to our prediction model. The columns are:
#1. Product dimensions (width, Depth and Height)
#2. Best Sellers Rank
#3. Shipping Weight
#4. Product Num (we don't want the id to add 'noise')
#We proceed to get rid of the aforementioned columns in both df (they don't need to be the same for prediction, but it's good practice)
ExistProd <- dplyr::select(ExistProd,-c(ProductNum,BestSellersRank,ShippingWeight,ProductDepth, ProductWidth, ProductHeight))
NewProd  <- dplyr::select(NewProd,-c(ProductNum,BestSellersRank,ShippingWeight,ProductDepth, ProductWidth, ProductHeight))
#BAD EXAMPLE: ExistProd <- ExistProd[,-c(2,12:16)]
#NewProd    <- NewProd[,-c(2,12:16)]
knitr::opts_chunk$set(echo = TRUE)
#We find lots of outliers in almost all columns. If we remove them, it will drastically change our dataset (already very small), and so we don't remove all of them.
ggplot(stack(ExistProd), aes(x=ind, y=values)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45))
#We find lots of outliers in almost all columns. If we remove them, it will drastically change our dataset (already very small), and so we don't remove all of them.
ggplot(stack(ExistProd), aes(x=ind, y=values)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45), hjust = 1)
#We find lots of outliers in almost all columns. If we remove them, it will drastically change our dataset (already very small), and so we don't remove all of them.
ggplot(stack(ExistProd), aes(x=ind, y=values)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
#But we should look more in depth at least in the outliers of the dependent column
ggplot(ExistProd, aes(x=ProductType, y=Volume)) +geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
compare.model
knitr::opts_chunk$set(echo = TRUE)
# Load libraries
#install.packages("pacman")
library(pacman)
library(caret)
library(lattice)
library(readr)
library(Metrics)
library(corrplot)
library(e1071)
library(mlr)
library(recipes)
library(ggplot2)
library(C50)
library(party)
library(reshape)
library(dplyr)
library(markdown)
library(ggpubr)
library(tidyr)
library(hydroGOF)
library(BBmisc)
library(tidyverse)
library(textclean)
library(inum)
library(doParallel)
library(Hmisc)
library(caretEnsemble)
library(mboost)
library(rstudioapi)
library(cluster)
library(ade4)
library(factoextra)
library(asbio)
#library(SatMatch)
library(FactoMineR)
library(fpc)
library(e1071)
library(randomForest)
library(MASS)
library(ParamHelpers)
library(mlr)
# Enable parallel computing
cl <- makePSOCKcluster(4)
registerDoParallel(cl)
# Loading dataset(s)
#current_path = getActiveDocumentContext()$path
setwd("C:/Users/Paul/Desktop/Ubiqum/Module 2/2.3 Multiple Regression/Git/Profitability")
ExistProd   <- read.csv("existingproductattributes2017.csv")
NewProd     <- read.csv("newproductattributes2017.csv")
NewProd_Raw <- read.csv("newproductattributes2017.csv")
#Because we have worked with these products before, and because the dataset is small, we can observe duplicates just by looking at the df itself. We find out all 'ExtendedWarranty' products share the same values in everything (including exact number of reviews of all types) except for a small different in price. In a business sense, We conclude this is because warranties are sold alongside different products, only the same type of warranty is included, but will vary in price proportionally to the price itself. We should treat them as the same products and remove all but one.
#Showing the 6 duplicates:
#IMPORTANT NOTE: It's good practice to remove the columns by name instead of by column number, so that if we do changes in dataset later, and we reload these commands, we won't mess up unwanted columns.
sum(duplicated(ExistProd[,c(1,4:18)]))
#Removing them, but setting an average price for the remaining product of Warranty:
#ExistProd <- ExistProd[-c(35:41),]
Duplicates <- ExistProd[c(32:41),]
Duplicates[3,3] <- sum(Duplicates[3:10,3])/8
Duplicates <- Duplicates[1:3,]
ExistProd  <- ExistProd[-c(32:41),]
ExistProd  <- rbind(ExistProd, Duplicates)
#We checked in the previous project that some columns were not useful and just added noise to our prediction model. The columns are:
#1. Product dimensions (width, Depth and Height)
#2. Best Sellers Rank
#3. Shipping Weight
#4. Product Num (we don't want the id to add 'noise')
#We proceed to get rid of the aforementioned columns in both df (they don't need to be the same for prediction, but it's good practice)
ExistProd <- dplyr::select(ExistProd,-c(ProductNum,BestSellersRank,ShippingWeight,ProductDepth, ProductWidth, ProductHeight))
NewProd  <- dplyr::select(NewProd,-c(ProductNum,BestSellersRank,ShippingWeight,ProductDepth, ProductWidth, ProductHeight))
#BAD EXAMPLE: ExistProd <- ExistProd[,-c(2,12:16)]
#NewProd    <- NewProd[,-c(2,12:16)]
#We find lots of outliers in almost all columns. If we remove them, it will drastically change our dataset (already very small), and so we don't remove all of them.
ggplot(stack(ExistProd), aes(x=ind, y=values)) + geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
#But we should look more in depth at least in the outliers of the dependent column
ggplot(ExistProd, aes(x=ProductType, y=Volume)) +geom_boxplot() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
#We have few outliers here as well. The R Built-in function will tell us the values.
boxplot(ExistProd$Volume, plot = FALSE)$out
#But we only want to remove the two extreme outliers because our dataset is small, and because these outliers don't belong to the product types we want to predict.
ExistProd <- ExistProd[-which(ExistProd$Volume > 7000),]
#Since different variables have different units, we are going to scale all numeric variables (except label):
for (i in c(2:11)){ExistProd[,i] <- scale(ExistProd[,i])}
#We do the same in NewProd df
for (i in c(2:11)){NewProd[,i] <- scale(NewProd[,i])}
#We are going to dummify(turn categorical values into logical ones)
#We need to dummify Product Type only. We check if it's a factor first
str(ExistProd$ProductType)
#The Correlation Matrix is only going to work with numeric values, so we create another df as to avoid messing with the current one, so that we can run a correlation matrix. We do this before dummifying, because once we dummify we'll have lots of variables and the correlation matrix will be way harder to read.
ExistProd_corr <- ExistProd[,-c(1)]
CorrData2 <- cor(ExistProd_corr)
corrplot(CorrData2, order="hclust", method = "number",
tl.col = "black", tl.srt = 90 , tl.cex = 0.7, tl.pos = "t")
#In order to find out the relationship between the label and the ProductType (categorical), we are going to use the ANOVA test.
ANOVA <- aov(ExistProd$Volume~ ExistProd$ProductType)
summary(ANOVA)
#Since the p-value is higher than 0.05, we reject the Null Hypothesis, which states that both variables come from the same population. Therefore, it's not useful to use this variable to predict Volume.
#Now we will confirm this with a Random Forest w/ dummyfeatures
ExistProd2 <- createDummyFeatures(obj = ExistProd, cols = "ProductType")
NewProd2   <- createDummyFeatures(obj = NewProd, cols = "ProductType")
set.seed(107)
inTrain  <- createDataPartition(ExistProd2$Volume,p = 0.75, list = FALSE)
training <- ExistProd2[inTrain,]
testing  <- ExistProd2[-inTrain,]
ctrl     <- trainControl(method = "cv", number = 10)
mRF  <- randomForest(Volume ~., ExistProd2, ntree = 80)
plot (mRF, main = "Performance of RF depending on the number of trees")
ctrl <- trainControl(method="repeatedcv",repeats = 3, number = 5) #,classProbs=TRUE,summaryFunction = twoClassSummary)
mRF2 <- caret::train(Volume ~ ., data = training, method = "rf", trControl = ctrl, preProcess = c("center","scale"), tuneLength = 2)
varImpPlot(mRF, main = "Performance of RF depending on the number of trees")
#As a conclusion, we get rid of Product Type
ExistProd$ProductType <- NULL
#ExistProd2 <- createDummyFeatures(obj = ExistProd, cols = "ProductType")
#NewProd2   <- createDummyFeatures(obj = NewProd, cols = "ProductType")
#We now look for collinearity in the newly dummified df
CorrData <- cor(ExistProd)
CorNames <- findCorrelation(CorrData, cutoff = 0.9, verbose = TRUE, names = TRUE)
#The above function recommends what columns we should erase based on collinearity. So we proceed to do that:
ExistProd <- ExistProd[, - which(names(ExistProd) %in% CorNames)]
#BAD PRACTICE
#ExistProd[ ,c(2,4,5)] <- list(NULL)
#NewProd[ ,c(2,4,5)]   <- list(NULL)
set.seed(107)
inTrain  <- createDataPartition(ExistProd2$Volume,p = 0.75, list = FALSE)
training <- ExistProd2[inTrain,]
testing  <- ExistProd2[-inTrain,]
ctrl     <- trainControl(method = "cv", number = 10)
nrow(training)
nrow(testing)
#The chosen models are looped below
Five_Models   <- c("lm", "rf", "knn", "svmLinear", "gbm")
#We create an empty variable so we can later assign all results to it
compare.model <- c()
#We run all models at once and assign the results in 'compare.model'.
#However, now the loop is overwriting each model as it goes through them, so it will only keep the last one saved.
for (i in Five_Models){
model         <- caret::train(Volume ~., data = training, method = i, )
pred          <- predict(model, newdata = testing)
pred.metric   <- postResample(testing$Volume, obs = pred)
compare.model <- cbind(pred.metric, compare.model)
}
#That's why we create a list with all models so that we can later choose the model
methods <- list()
#We will save them in the variable previously created
for (i in Five_Models) {
methods[[i]] <- caret::train(Volume ~., data = training, method = i )
}
methods
colnames(compare.model) <- Five_Models
compare.model
compare.model.melt <- melt(compare.model, varnames = c("metric", "model"))
compare.model.melt <- as.data.frame(compare.model.melt)
compare.model.melt
#All models offer quite different results, which indicates the predictions will have high variance between models.
#Based on errors, the SVM stands out in MAE (smaller error than others). RMSE is pretty similar in all. As for the RSquared, RF performes better than SVM here, but SVM comes in second, so we choose this model.
for(i in c("RMSE","Rsquared","MAE")) {
metric <-  compare.model.melt %>%  filter(metric == i)
gg     <- ggplot(metric, aes(model,value))
print(gg + geom_bar(stat = "identity") + ggtitle(i))
}
predictions_svm <- predict(methods$svmLinear, NewProd2)
predictions_svm
#Adding our predictions to the test set
Prod_Sales_Pred                 <- NewProd_Raw[,-c(3:nrow(NewProd_Raw))]
Prod_Sales_Pred$predictions_svm <- predictions_svm
Prod_Sales_Pred                 <- Prod_Sales_Pred[-c(10:11,16:24),]
Prod_Sales_Pred$ProductNum <- mgsub(x = Prod_Sales_Pred$ProductNum,pattern = c(171,172,173,175,176,178,180,181,183,193,194,195,196), replacement = c("Dell1","Dell2","Apple","Toshiba","Razer","hp","Acer","Asus","Samsung1","Motorola1","Samsung2","HTC","Motorola2"))
#We can observe how sales vary among the different products
ggplot(Prod_Sales_Pred, aes(x=ProductNum,y=predictions_svm, fill=ProductType)) + geom_col() + theme(axis.text.x = element_text(angle = 45, hjust = 1))
View(Prod_Sales_Pred)
tinytex::install_tinytex()
library(tinytex)
#disable scientific notation
options(scipen = 999)
ExistProd   <- read.csv("existingproductattributes2017.csv")
install.packages("ghostscript")
install.packages("pdfcrop")
install.packages("pdfcrop")
tinytex::tlmgr_install("pdfcrop")
install.packages('tinytex')
install.packages("tinytex")
